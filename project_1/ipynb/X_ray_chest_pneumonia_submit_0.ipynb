{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X-ray chest - pneumonia detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from glob import glob\n",
    "import shutil\n",
    "import random\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "\n",
    "# Graphic\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "##Import any other stats/DL/ML packages you may need here. E.g. Keras, scikit-learn, etc.\n",
    "#Keras (https://keras.io/api/metrics/)\n",
    "#from tensorflow.keras import layers\n",
    "#scikit-learn (https://scikit-learn.org/stable/)\n",
    "# sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score,recall_score, precision_score, confusion_matrix, f1_score,classification_report,precision_recall_curve\n",
    "#from f1_score import calc_f1\n",
    "\n",
    "\n",
    "# Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Do some early processing of your metadata for easier model training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. add new column with the path to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans found: 112120 , Total Headers 112120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78968</th>\n",
       "      <td>00019373_055.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>55</td>\n",
       "      <td>19373</td>\n",
       "      <td>38</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/images_009/images/00019373_055.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50942</th>\n",
       "      <td>00012880_004.png</td>\n",
       "      <td>Atelectasis</td>\n",
       "      <td>4</td>\n",
       "      <td>12880</td>\n",
       "      <td>64</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2992</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/images_006/images/00012880_004.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80787</th>\n",
       "      <td>00019860_005.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>5</td>\n",
       "      <td>19860</td>\n",
       "      <td>52</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/images_009/images/00019860_005.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Image Index Finding Labels  Follow-up #  Patient ID  Patient Age  \\\n",
       "78968  00019373_055.png     No Finding           55       19373           38   \n",
       "50942  00012880_004.png    Atelectasis            4       12880           64   \n",
       "80787  00019860_005.png     No Finding            5       19860           52   \n",
       "\n",
       "      Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "78968              M            AP                 2500     2048   \n",
       "50942              M            PA                 2992     2991   \n",
       "80787              M            AP                 2500     2048   \n",
       "\n",
       "       OriginalImagePixelSpacing[x     y]  Unnamed: 11  \\\n",
       "78968                        0.168  0.168          NaN   \n",
       "50942                        0.143  0.143          NaN   \n",
       "80787                        0.168  0.168          NaN   \n",
       "\n",
       "                                           path  \n",
       "78968  /data/images_009/images/00019373_055.png  \n",
       "50942  /data/images_006/images/00012880_004.png  \n",
       "80787  /data/images_009/images/00019860_005.png  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Below is some helper code to read all of your full image filepaths into a dataframe for easier manipulation\n",
    "## Load the NIH data to all_xray_df\n",
    "all_xray_df = pd.read_csv('/data/Data_Entry_2017.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join('/data','images*', '*', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Add coumn for each decease from the 'Finding Labels' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here you may want to create some extra columns in your table with binary indicators of certain diseases\n",
    "## rather than working directly with the 'Finding Labels' column\n",
    "##\n",
    "finding_labels = set('|'.join(all_xray_df['Finding Labels']).split('|'))\n",
    "\n",
    "for label in finding_labels:\n",
    "  all_xray_df[label] = all_xray_df['Finding Labels'].apply(lambda x: 1 if label in x else 0)\n",
    "#drop Findings label once decease has been split in different columns\n",
    "all_xray_df = all_xray_df.drop(['Finding Labels'], axis=1)\n",
    "#drop usless columm \n",
    "all_xray_df = all_xray_df.drop(['Unnamed: 11'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Image Index', 'Follow-up #', 'Patient ID', 'Patient Age',\n",
       "       'Patient Gender', 'View Position', 'OriginalImage[Width', 'Height]',\n",
       "       'OriginalImagePixelSpacing[x', 'y]', 'path', 'Effusion', 'Atelectasis',\n",
       "       'Pleural_Thickening', 'Pneumothorax', 'Mass', 'No Finding', 'Edema',\n",
       "       'Consolidation', 'Hernia', 'Infiltration', 'Nodule', 'Pneumonia',\n",
       "       'Cardiomegaly', 'Fibrosis', 'Emphysema'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_headers = all_xray_df.columns\n",
    "column_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Add a column with the number of decease per record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_columns = finding_labels.copy()\n",
    "disease_columns.remove('No Finding')\n",
    "all_xray_df['disease_number']=all_xray_df[disease_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate wrong input detected within the EDA phase but also patient aged more than 60 years with high risk of complication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age < 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85072\n"
     ]
    }
   ],
   "source": [
    "all_xray_df = all_xray_df[all_xray_df['Patient Age']<60]\n",
    "print(len(all_xray_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eliiminate Images with outlier formats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width:  [array([2500., 2500., 2992., 2992., 2500.])]\n",
      "height:  [array([2048., 2048., 2991., 2991., 2048.])]\n",
      "opx: [array([0.143, 0.143, 0.168, 0.168, 0.143])]\n",
      "opy: [array([0.143, 0.143, 0.168, 0.168, 0.143])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(4,1,figsize=(15,15))\n",
    "b_dict_w= ax[0].boxplot([all_xray_df['OriginalImage[Width']],vert=False)\n",
    "b_dict_h= ax[1].boxplot([all_xray_df['Height]']],vert=False)\n",
    "b_dict_opx= ax[2].boxplot([all_xray_df['OriginalImagePixelSpacing[x']],vert=False)\n",
    "b_dict_opy= ax[3].boxplot([all_xray_df['y]']],vert=False)\n",
    "\n",
    "boxes_w = [box.get_xdata() for box in b_dict_w['boxes']]\n",
    "boxes_h = [box.get_xdata() for box in b_dict_h['boxes']]\n",
    "boxes_opx = [box.get_xdata() for box in b_dict_opx['boxes']]\n",
    "boxes_opy = [box.get_xdata() for box in b_dict_opy['boxes']]\n",
    "\n",
    "print(f\"width:  {boxes_w}\")\n",
    "print(f\"height:  {boxes_h}\")\n",
    "print(f\"opx: {boxes_opx}\")\n",
    "print(f\"opy: {boxes_opy}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966.0 - 3056.0\n"
     ]
    }
   ],
   "source": [
    "whiskers_w = [whisker.get_xdata() for whisker in b_dict_w['whiskers']]\n",
    "w_low=whiskers_w[0][1]\n",
    "w_high=whiskers_w[1][1]\n",
    "#print(f'{w_low} - {w_high}')\n",
    "whiskers_h = [whisker.get_xdata() for whisker in b_dict_h['whiskers']]\n",
    "h_low=whiskers_h[0][1]\n",
    "h_high=whiskers_h[1][1]\n",
    "print(f'{h_low} - {h_high}')\n",
    "all_xray_df = all_xray_df[all_xray_df['OriginalImage[Width']>w_low]\n",
    "all_xray_df = all_xray_df[all_xray_df['OriginalImage[Width']<w_high]\n",
    "all_xray_df = all_xray_df[all_xray_df['Height]']>h_low]\n",
    "all_xray_df = all_xray_df[all_xray_df['Height]']<h_high]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Case data splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(all_xray_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82663"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Get all normal cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodisease_df = all_xray_df[all_xray_df['No Finding']==1]\n",
    "nodisease_data_size = len( nodisease_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Get diseased cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_df = all_xray_df[all_xray_df['No Finding']==0]\n",
    "disease_data_size = len(disease_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Get other disease cases without pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_wo_pneumonia_df = disease_df[disease_df.Pneumonia==0]\n",
    "disease_wo_pneumonia_data_size = len(disease_wo_pneumonia_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Get pneumonia cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_pneumonia_df= disease_df[disease_df.Pneumonia==1]\n",
    "disease_pneumonia_data_size = len(disease_pneumonia_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. Get pure pneumonial cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_pure_df= disease_pneumonia_df[disease_pneumonia_df.disease_number<2]\n",
    "pneumonia_pure_data_size = len(pneumonia_pure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. Get pneumonia with other disease case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_other_df = disease_pneumonia_df[disease_pneumonia_df.disease_number>1]\n",
    "pneumonia_other_data_size = len(pneumonia_other_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy cases                 : 45909\n",
      "Decease cases                 : 36754\n",
      "  Not pneumonia cases         :   35688\n",
      "  Pneumonia cases             :   1066\n",
      "    -Pure cases               :     255\n",
      "    -With other deceases cases:     811\n",
      "Total cases                   : 82663\n"
     ]
    }
   ],
   "source": [
    "print(f'Healthy cases                 : {nodisease_data_size}')\n",
    "print(f'Decease cases                 : {disease_data_size}')\n",
    "print(f'  Not pneumonia cases         :   {disease_wo_pneumonia_data_size}') \n",
    "print(f'  Pneumonia cases             :   {disease_pneumonia_data_size}')\n",
    "print(f'    -Pure cases               :     {pneumonia_pure_data_size}')\n",
    "print(f'    -With other deceases cases:     {pneumonia_other_data_size}')\n",
    "print(f'Total cases                   : {data_size}' )      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create your training and testing data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have 2 categories:\n",
    "\n",
    " - Negative (Healthy or other diseases) = 0\n",
    " - Pneumonia (with/without other diseases) = 1\n",
    "\n",
    "Because of the relatively few case of pneumonia case (1430) , we will augment using **geometrical** transformation using Keras data generator to some extends to have at least a ratio of 1/10 of the healthy cases. Since we don't have acces to radiologist to label image and we were not successful to extract distinct patterns when analysing image pixels , we won't use custom augmmentation by blurring or somoothing images.  \n",
    "\n",
    "On the other side we will tray to reduce by balancing not pneumonial case cross gender and age.\n",
    "\n",
    "Once, this augmentation done , we will initializing a convolutional neural network (CNN) with class weights calculate with the following formulas:\n",
    "\n",
    "weight_class_x = (1 / COUNT_CLASS_X) * (TOTAL_SAMPLES) / n\n",
    "\n",
    "n= number of class\n",
    "x = [1....n]\n",
    "\n",
    "Note : due to technical no resolved issues in our environment, unfortunately we can't run all the generation in the same session at onces without the kernel beeing interupted will be interrupted  , we 'll generate each category separrately within files to prepare the training and testing phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodecease_df=nodecease_df.drop(columns=['category'])\n",
    "#decease_wo_pneumonia_df=decease_wo_pneumonia_df.drop(columns=['category'])\n",
    "#pneumonia_pure_df=pneumonia_pure_df.drop(columns=['category'])\n",
    "#pneumonia_other_df=pneumonia_other_df.drop(columns=['category'])\n",
    "#nodecease_df['category']\n",
    "categories = ['NO_PNEUMONIA','PNEUMONIA'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:844: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#nodisease_df=nodecease_df.copy()\n",
    "nodisease_df.loc[:, 'category'] = categories[0]\n",
    "#disease_wo_pneumonia_df= disease_wo_pneumonia_df.copy()\n",
    "disease_wo_pneumonia_df.loc[:, 'category'] = categories[0]\n",
    "\n",
    "pneumonia_pure_df = pneumonia_pure_df.copy()\n",
    "pneumonia_pure_df.loc[:,'category']=categories[1]\n",
    "pneumonia_other_df = pneumonia_other_df.copy()\n",
    "pneumonia_other_df.loc[:,'category']=categories[1]\n",
    "#pneumonia_all_df= pd.concat([pneumonia_pure_df,pneumonia_other_df],ignore_index=True)\n",
    "#shuffle\n",
    "#pneumonia_all_df = pneumonia_all_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#not rally nice code , but we have to merge the pneumonia case for technical issue when trainig \n",
    "#w_p_o = len(pneumonia_other_df)\n",
    "#w_p_p = len(pneumonia_pure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.0. Helper's Functions and infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to append file paths to a specific category and type of data in the dictionary\n",
    "\n",
    "'''\n",
    "def append_to_paths_dictonary(paths_dic, data_type,category, path):\n",
    "    if data_type in paths_dic:\n",
    "        if category in paths_dic[data_type]:\n",
    "            print(f'append - {category}.{data_type}')\n",
    "            paths_dic[data_type][category].append(path)\n",
    "        else:\n",
    "            print(f'new - {data_type}.{category}')\n",
    "            paths_dic[data_type][category] = [path]\n",
    "    else:\n",
    "        paths_dic[data_type] = {category: [path]}\n",
    "        \n",
    "'''\n",
    "Function to access the filepath in the paths_dict with the category and type of data as parameter\n",
    "'''\n",
    "def get_from_paths_dictonary(paths_dic,data_type,category):\n",
    "    if data_type in paths_dic and category in paths_dic[data_type]:\n",
    "        return paths_dic[data_type][category]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_from_paths_type(paths_dic,data_type): \n",
    "    if data_type in paths_dic:\n",
    "        for key, value in paths_dic[data_type].items():\n",
    "            return(os.path.dirname(value[0]))\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "'''\n",
    "Function to get all file paths from the dictionary\n",
    "'''    \n",
    "def get_all_paths_from_paths_dictionary(paths_dic):\n",
    "    all_paths = []\n",
    "    for type,categories in paths_dic.items():\n",
    "        for file_type, paths in categories.items():\n",
    "            all_paths.extend(paths)\n",
    "    return all_paths    \n",
    "\n",
    "def remove_directory(path):\n",
    "    \"\"\"\n",
    "    Removes a directory and its contents.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the directory to be removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Directory '{path}' and its contents removed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing directory '{path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new - train.PNEUMONIA\n",
      "new - val.PNEUMONIA\n",
      "Directory '/workspace/home/generated/train/NO_PNEUMONIA' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/train/NO_PNEUMONIA' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/train/PNEUMONIA' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/train/PNEUMONIA' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/val/NO_PNEUMONIA' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/val/NO_PNEUMONIA' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/val/PNEUMONIA' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/val/PNEUMONIA' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/train/NO_PNEUMONIA' and its contents created successfully.\n",
      "Directory '/workspace/home/generated/train/PNEUMONIA' and its contents created successfully.\n",
      "Directory '/workspace/home/generated/val/NO_PNEUMONIA' and its contents created successfully.\n",
      "Directory '/workspace/home/generated/val/PNEUMONIA' and its contents created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Initialization procedure\n",
    "import os\n",
    "\n",
    "generated_root_dir = '/workspace/home/generated'\n",
    "\n",
    "# Create the main directory\n",
    "os.makedirs(generated_root_dir, exist_ok=True)\n",
    "\n",
    "# Initialize an empty directory for generated paths for each category and type of data\n",
    "generated_file_paths_dic = {}\n",
    "\n",
    "#generate filepaths directory names\n",
    "for category in categories:\n",
    "    #test_path = f'{generated_root_dir}/test/{category}'\n",
    "    #append_to_paths_dictonary(generated_file_paths_dic,'test',category,test_path)\n",
    "    train_path = f'{generated_root_dir}/train/{category}'\n",
    "    append_to_paths_dictonary(generated_file_paths_dic,'train',category,train_path)\n",
    "    val_path = f'{generated_root_dir}/val/{category}'\n",
    "    append_to_paths_dictonary(generated_file_paths_dic,'val',category,val_path)\n",
    "\n",
    "    \n",
    "    \n",
    "paths_to_create = get_all_paths_from_paths_dictionary(generated_file_paths_dic)\n",
    "\n",
    "# clean\n",
    "regenerate=False\n",
    "if regenerate:\n",
    "    for path  in paths_to_create:\n",
    "        remove_directory(path)\n",
    "        print(f\"Directory '{path}' and its contents removed successfully.\")\n",
    "\n",
    "\n",
    "    #Create subdirectories\n",
    "    for path  in paths_to_create:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"Directory '{path}' and its contents created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.0.1 Image generator function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.0.2 Prepare training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as skl\n",
    "\n",
    "def prepare_data(df,criteria , test_size = 0.2):\n",
    "    train_df, valid_df = skl.train_test_split(df, \n",
    "                                   test_size = test_size, \n",
    "                                   stratify = df[criteria] if criteria is not None else None)\n",
    "    \n",
    "    if criteria is not None:\n",
    "        criteria_idxs = df[df[criteria]==1].index.tolist()\n",
    "        not_criteria_idxs = df[df[criteria]==0].index.tolist()\n",
    "        print('train percentage',len(train_df)/len(df))\n",
    "        print('validation percentage',len(valid_df)/len(df))\n",
    "        criteria_len = len(criteria_idxs)\n",
    "        not_criteria_len = len(not_criteria_idxs)\n",
    "    \n",
    "        percent_critera = criteria_len/len(df)\n",
    "        percent_not_critera = not_criteria_len/len(df)\n",
    "\n",
    "        print(f'percent criteria {criteria}: {percent_critera}')\n",
    "        print(f'percent non criteria {criteria}: {percent_not_critera}')\n",
    "    train_df_size=len(train_df)\n",
    "    valid_df_size=len(valid_df)\n",
    "    print(f'train data size: {train_df_size}')\n",
    "    print(f'valid data size: {valid_df_size}')\n",
    "    return train_df,train_df_size,valid_df,valid_df_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%more Data_Entry_2017.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transform_images(data_generator,df,src_dir,tgt_dir,factor,path,category_fieldname, data_type = 'train'):\n",
    "    print(f'start images transfomation and generation...')\n",
    "    n=0\n",
    "    for index, row in df.iterrows():\n",
    "        image_path = os.path.join(src_dir, row[path])\n",
    "        category = row[category_fieldname]\n",
    "\n",
    "        img = load_img(image_path)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        i = 0\n",
    "        # Generate a unique filename using current timestamp and a counter\n",
    "        timestamp = int(time.time())\n",
    "        save_to_dir = os.path.join(tgt_dir, data_type, category)\n",
    "        \n",
    "\n",
    "        for batch in data_generator.flow(x, batch_size=1, save_to_dir=save_to_dir, save_format='jpeg'):\n",
    "            i += 1\n",
    "            if i >= factor: \n",
    "                break\n",
    "        n += i \n",
    "    print(f'end images transfomation and generation {n}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3.Balance and generate of other decease and healthy cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.1 Function for data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Replace 'your_dataset.csv' with your dataset file path and specify the column names and total_rows_to_select.\n",
    "# balanced_data = balance_dataset(df, ['age', 'gender'], 100)\n",
    "def select_balanced_dataset(df, column_names, total_rows_to_select):\n",
    "    # Calculate the total number of rows you want to select\n",
    "    num_groups = df.groupby(column_names).size().shape[0]\n",
    "    min_records_per_group = round(total_rows_to_select / num_groups)\n",
    "\n",
    "    # Initialize an empty DataFrame to store the sampled rows\n",
    "    balanced_df = pd.DataFrame(columns=df.columns)\n",
    "    selected_indexes = []\n",
    "    groupes_count = {}\n",
    "    # Randomly sample from each group\n",
    "    for _, group in df.groupby(column_names):\n",
    "        count = group.shape[0]\n",
    "        if count >= min_records_per_group:\n",
    "            # If the group has enough records, sample the required number\n",
    "            sample = group.sample(min_records_per_group, random_state=42)\n",
    "        else:\n",
    "            # If the group has fewer records than required, sample all of them\n",
    "            sample = group    \n",
    "        # Add the sampled rows to the balanced DataFrame\n",
    "        balanced_df = pd.concat([balanced_df, sample])\n",
    "\n",
    "        #store the index\n",
    "        selected_indexes.extend(sample.index)\n",
    "        \n",
    "    # If you want a completely random subset, shuffle the resulting DataFrame\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42)\n",
    "\n",
    "    return balanced_df , selected_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.2. Balanced selection of other than pneumonia desease cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickup  ~ 2 * generated pneumonia ( ~10000 training case and 3000 test case) ->  25000 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_number = 8000//4\n",
    "balanced_disease_wo_pneumonia_df,selected_index = select_balanced_dataset(disease_wo_pneumonia_df, column_names = ['Patient Age','Patient Gender','View Position' ], total_rows_to_select = selection_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_disease_wo_pneumonia_df size: 2020\n"
     ]
    }
   ],
   "source": [
    "print(f'balanced_disease_wo_pneumonia_df size: {len(balanced_disease_wo_pneumonia_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if balanced cross Gender,age and view postion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1016.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        1004.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(balanced_disease_wo_pneumonia_df['Patient Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([118., 210., 216., 216., 180., 216., 216., 216., 216., 216.]),\n",
       " array([1.0, 6.8, 12.6, 18.4, 24.2, 30.0, 35.8, 41.6, 47.4,\n",
       "        53.199999999999996, 59.0], dtype=object),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(balanced_disease_wo_pneumonia_df['Patient Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([118., 210., 216., 216., 180., 216., 216., 216., 216., 216.]),\n",
       " array([1.0, 6.8, 12.6, 18.4, 24.2, 30.0, 35.8, 41.6, 47.4,\n",
       "        53.199999999999996, 59.0], dtype=object),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(balanced_disease_wo_pneumonia_df['Patient Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** there to few sample for age < 20 and >=80 to have a perfect equal rebalancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1033.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "         987.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(balanced_disease_wo_pneumonia_df['View Position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.3. Balanced selection of healthy cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_number = 8000//4\n",
    "balanced_nodisease_df,selected_index = select_balanced_dataset(nodisease_df, column_names = ['Patient Age','Patient Gender','View Position' ], total_rows_to_select = selection_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_nodisease_df: 1856\n"
     ]
    }
   ],
   "source": [
    "print(f'balanced_nodisease_df: {len(balanced_nodisease_df)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.5. Training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 204\n",
      "valid data size: 51\n",
      "train data size: 648\n",
      "valid data size: 163\n",
      "train data size: 1484\n",
      "valid data size: 372\n",
      "train data size: 1616\n",
      "valid data size: 404\n"
     ]
    }
   ],
   "source": [
    "train_pneumonia_pure,train_pneumonia_pure_size,valid_pneumonia_pure,valid_pneumonia_pure_size =prepare_data(pneumonia_pure_df,None , test_size = 0.2)\n",
    "train_pneumonia_other,train_pneumonia_other_size,valid_pneumonia_other,valid_pneumonia_other_size =prepare_data(pneumonia_other_df,None , test_size = 0.2)\n",
    "train_nodisease,train_nodisease_size,valid_nodisease,valid_nodisease_size =prepare_data(balanced_nodisease_df,None , test_size = 0.2)\n",
    "train_disease_wo_pneumonia,train_disease_wo_pneumonia_size,valid_disease_wo_pneumonia,valid_disease_wo_pneumonia_size =prepare_data(balanced_disease_wo_pneumonia_df,None , test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pneumonia = pd.concat([valid_nodisease,valid_disease_wo_pneumonia],ignore_index=True)\n",
    "no_pneumonia = no_pneumonia.sample(n= (valid_pneumonia_other_size+valid_pneumonia_pure_size))                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.4. Initialize parameter for images generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale= 1./255\n",
    "h_flip=True\n",
    "h_shift_range=3\n",
    "w_shift_range=3\n",
    "rotation_range=2\n",
    "#zoom_range=0.0\n",
    "#brightness_range=[0.4,1.5]\n",
    "classes = categories\n",
    "class_mode ='categorical'\n",
    "x_col = 'path'\n",
    "y_col = 'category'\n",
    "\n",
    "tgt_size = (224,224)\n",
    "batch_size = 128\n",
    "# generator with modification and rescale\n",
    "img_mod_gen = ImageDataGenerator(rescale=rescale,\n",
    "                             horizontal_flip = h_flip,\n",
    "                             height_shift_range = h_shift_range,\n",
    "                             width_shift_range = w_shift_range,\n",
    "                             rotation_range = rotation_range)\n",
    " #                            brightness_range= brightness_range,      \n",
    " #                            fill_mode='nearest',\n",
    " #                            zoom_range = zoom_range,\n",
    " #                            fill_mode='nearest')\n",
    "# generator with rescale and withoud modification\n",
    "img_ori_gen = ImageDataGenerator(rescale=rescale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.5 Augmentation  pneumonia cases generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start images transfomation and generation...\n",
      "end images transfomation and generation 2556.\n",
      "start images transfomation and generation...\n",
      "end images transfomation and generation 214.\n",
      "[gc]count (73, 9, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9202"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#factor = 4 \n",
    "factor = 3 \n",
    "# Generate training pure pneumonia images\n",
    "#regenerate = True\n",
    "#regenerate = False\n",
    "\n",
    "if regenerate:\n",
    "    generate_transform_images(img_mod_gen,pd.concat([train_pneumonia_pure,train_pneumonia_other],ignore_index=True), '/', generated_root_dir,factor , 'path','category')\n",
    "    # Generate validation pure pneumonia images\n",
    "    generate_transform_images(img_ori_gen,pd.concat([valid_pneumonia_pure,valid_pneumonia_other],ignore_index=True),'/', generated_root_dir,1 , 'path','category',data_type = 'val')\n",
    "print(f'[gc]count {gc.get_count()}')\n",
    "gc.collect()                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generated_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home/generated/train/PNEUMONIA\n",
      "2290\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/home/generated/train/PNEUMONIA\n",
    "%ls -1p | grep -v / | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home/generated/val/PNEUMONIA\n",
      "213\r\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/home/generated/val/PNEUMONIA\n",
    "%ls -1p | grep -v / | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.7 Augmentation  no disease cases generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start images transfomation and generation...\n",
      "end images transfomation and generation 3100.\n",
      "start images transfomation and generation...\n",
      "end images transfomation and generation 214.\n",
      "[gc]count (179, 6, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor = 1\n",
    "\n",
    "if regenerate:\n",
    "    # Generate training healthy images\n",
    "    # use img_val_gen , becuase we don't want generation \n",
    "    generate_transform_images(img_ori_gen,pd.concat([train_nodisease,train_disease_wo_pneumonia],ignore_index=True),'/', generated_root_dir,factor , 'path','category')\n",
    "    # Generate validation healthy without pneumonia images\n",
    "    generate_transform_images(img_ori_gen,no_pneumonia,'/', generated_root_dir,factor , 'path','category',data_type = 'val')\n",
    "print(f'[gc]count {gc.get_count()}')\n",
    "gc.collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home/generated/train/NO_PNEUMONIA\n",
      "2669\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/home/generated/train/NO_PNEUMONIA\n",
    "%ls -1p | grep -v / | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home/generated/val/NO_PNEUMONIA\n",
      "214\r\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/home/generated/val/NO_PNEUMONIA\n",
    "%ls -1p | grep -v / | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home\n"
     ]
    }
   ],
   "source": [
    "# reinit\n",
    "%cd  /workspace/home/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2.3.5 generator for training and validation based on previously created images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory: /workspace/home/generated/train\n",
      "Validation directory: /workspace/home/generated/val\n"
     ]
    }
   ],
   "source": [
    "train_dir = get_from_paths_type(generated_file_paths_dic,'train')\n",
    "print(f'Training directory: {train_dir}')\n",
    "val_dir = get_from_paths_type(generated_file_paths_dic,'val')\n",
    "print(f'Validation directory: {val_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4959 images belonging to 2 classes.\n",
      "Found 427 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "             directory=train_dir,\n",
    "             batch_size=batch_size,\n",
    "             target_size = tgt_size,\n",
    "             class_mode='categorical')\n",
    "\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "             directory=val_dir,\n",
    "             batch_size=batch_size//2,\n",
    "             target_size = tgt_size,\n",
    "             class_mode='categorical')\n",
    "\n",
    "# Number of train and validation steps\n",
    "train_steps=train_data.n//batch_size\n",
    "val_steps=val_data.n//(batch_size//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val_1, Y_val_1 = valid_gen_1.next()\n",
    "#X_val_2, Y_val_2 = valid_gen_2.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4  Model-building & training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First suggestion: perform some image augmentation on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May want to look at some examples of our augmented training data. \n",
    "## This is helpful for understanding the extent to which data is being manipulated prior to training, \n",
    "## and can be compared with how the raw data look prior to augmentation\n",
    "show_image = False\n",
    "if show_image:\n",
    "    t_x, t_y = next(train_gen)\n",
    "    fig, m_axs = plt.subplots(4, 2, figsize = (20, 20))\n",
    "    for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "        c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "        print(c_y)\n",
    "        if c_y[1] == 1: \n",
    "            c_ax.set_title('Pneumonia')\n",
    "        else:\n",
    "            c_ax.set_title('No Pneumonia')\n",
    "        c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Build your model: \n",
    "\n",
    "Recommendation here to use a pre-trained network downloaded from Keras for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense,Dropout,GlobalAveragePooling2D,AveragePooling2D,MaxPooling2D,Conv2D,Activation,Add,PReLU\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "#from keras.models import Sequential\n",
    "from tensorflow.python.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, filters):\n",
    "    # First Convolution layer\n",
    "    conv1 = Conv2D(filters, (3, 3), padding='same')(inputs)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "\n",
    "    # Second convolution layer\n",
    "    conv2 = Conv2D(filters, (3, 3), padding='same')(conv1)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "\n",
    "    # Attention Mechanism\n",
    "    attn = Add()([conv1, conv2])\n",
    "    attn = Activation('relu')(attn)\n",
    "    attn = Conv2D(1, (1, 1), padding='same')(attn)\n",
    "    attn = Activation('softmax')(attn)\n",
    "\n",
    "    # Multiply the input with the attention map\n",
    "    output = tf.multiply(inputs, attn)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_layers(x,dropout):\n",
    "    # Add dropout layer after Flatten layer\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "    #x = Dropout(dropout)(x)\n",
    "    #x = Dense(512, activation='relu', name='fc2')(x)\n",
    "    #x = Dropout(dropout)(x)\n",
    "\n",
    "    #x = PReLU()(x)  # The slope is learned during training\n",
    "    #x = Dropout(dropout)(x)  # Dropout rate can be adjusted\n",
    "    #x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architechture\n",
    "def create_simple(num_classes,node_number,dropout=0.24,input_shape= (224,224,3)):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_custom_VGG16(num_classes,node_number,dropout=0.5,input_shape= (224,224,3), num_trainable_layers=20,l2_reg = 1e-5):\n",
    "\n",
    "    # Load pre-trained VGG16 model (exclude top layers)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False,input_shape = input_shape)\n",
    "\n",
    "\n",
    "    # Apply attention block\n",
    "    # inpired by https://www.nature.com/articles/s41598-022-27266-9\n",
    "    #if attention:\n",
    "    #    x = attention_block(base_model.output, 64)  # Adjust filters as needed\n",
    "    #else:\n",
    "    x=  base_model.output\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Flatten(name='flattenCustom')(x) \n",
    "    x = Dense(node_number, activation='relu', name='fc1', kernel_regularizer=l2(l2_reg))(x)\n",
    "    #x = Dropout(dropout)(x)\n",
    "    # x = Dense(64,activation= 'relu',name='fc2')(x)\n",
    "    # Classification layer\n",
    "    predictions = Dense(num_classes, activation='softmax',name='fc3')(x)  # Replace 2 with your number of classes\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=base_model.inputs, outputs=predictions)\n",
    "    \n",
    "    \n",
    "    frozen_model_layers = model.layers\n",
    "    if num_trainable_layers> 0:\n",
    "        frozen_model_layers = model.layers[:-num_trainable_layers]\n",
    "    for layer in frozen_model_layers:\n",
    "        layer.trainable = False    # Freeze the specified number of layers\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "def create_custom_resnet50v2(num_classes,dropout=0.2,input_shape= (224,224,3), flattening = True,num_trainable_layers=4):\n",
    "    #base_model = ResNet50V2(weights='imagenet', include_top=False, pooling='avg',input_shape=input_shape)\n",
    "    base_model = None\n",
    "    if flattening:\n",
    "        base_model=ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        base_model=ResNet50V2(weights='imagenet', include_top=False,  pooling='avg',input_shape=input_shape)\n",
    "   \n",
    "    #base_model.trainable= False\n",
    "        \n",
    "    x = base_model.output\n",
    "\n",
    "    # Add dropout for regularization (optional)\n",
    "    x = Flatten(name='flattenCustom')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(512,activation='relu', name='fc1')(x)    \n",
    "    x = Dropout(dropout)(x)\n",
    "    #predictions = Dense(2, activation='softmax', name = 'fc2',kernel_initializer = glorot_uniform(seed=None))(x)  # Use a single neuron with sigmoid activation for binary classification\n",
    "    predictions = Dense(2, activation='softmax', name = 'fc2')(x)  # Use a single neuron with sigmoid activation for binary classification\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    frozen_model_layers = model.layers\n",
    "    if num_trainable_layers> 0:\n",
    "        frozen_model_layers = model.layers[:-num_trainable_layers]\n",
    "    for layer in frozen_model_layers:\n",
    "        layer.trainable = False    # Freeze the specified number of layers\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VGG16'\n",
    "#model_name = 'SIMPLE'\n",
    "#model_name = 'RESNET50V2'\n",
    "\n",
    "node_number = 256\n",
    "version    = '1.0'\n",
    "learning_rate  = 0.0001\n",
    "limit_rate     = 0.00001\n",
    "decrease_rate  = 0.1\n",
    "dropout = 0.5\n",
    "l2_reg = 1e-4 #, 1e-4, 1e-3, and 1e-2\n",
    "parameters = 'n{}_l_r_{}_l{}_dr_{}_l2_{}_d{}_b{}'.format(node_number,learning_rate,limit_rate,decrease_rate,l2_reg,dropout,batch_size,l2_reg = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channel_nbr = 3\n",
    "net= create_custom_VGG16(len(categories),node_number,dropout=dropout, input_shape = (tgt_size[0],tgt_size[1],channel_nbr),num_trainable_layers=6)\n",
    "\n",
    "#net = create_simple(len(categories),node_number,dropout=dropout,input_shape= (tgt_size[0],tgt_size[1],channel_nbr))\n",
    "\n",
    "#net = create_custom_resnet50v2(len(categories),dropout=0.2,input_shape = (tgt_size[0],tgt_size[1],channel_nbr),flattening=True,num_trainable_layers=5)\n",
    "# display the structure of the model\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in net.layers:\n",
    "    print(layer, layer.trainable)\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        print(f\"Layer Name: {layer.name}, Activation: {layer.activation.__name__}\")\n",
    "\n",
    "net.summary()\n",
    "print('{}_{}_{} generated'.format(model_name,version,parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to JSON-compatible dictionary\n",
    "model_json = net.to_json()\n",
    "# Serialize the dictionary to JSON format\n",
    "\n",
    "model_path = '/workspace/home/{}_{}_{}.json'.format(model_name,version,parameters)\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_size  = [2660, 2260]\n",
    "min_category_size = min(categories_size)\n",
    "categories_weights = [x/min_category_size for x in categories_size]\n",
    "class_weights_dict = dict(enumerate(categories_weights))\n",
    "print(f'class_weights_dict : {class_weights_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below is some helper code that will allow you to add checkpoints to your model,\n",
    "## This will save the 'best' version of your model by comparing it to previous epochs of training\n",
    "\n",
    "## Note that you need to choose which metric to monitor for your model's 'best' performance if using this code. \n",
    "## The 'patience' parameter is set to 10, meaning that your model will train for ten epochs without seeing\n",
    "## improvement before quitting\n",
    "\n",
    "\n",
    "weight_path_loss=\"/workspace/home/{}_{}_{}_loss.best.hdf5\".format(model_name,version,parameters)\n",
    "weight_path_acc=\"/workspace/home/{}_{}_{}_acc.best.hdf5\".format(model_name,version,parameters)\n",
    "\n",
    "checkpoint_1 = ModelCheckpoint(weight_path_loss, \n",
    "                             monitor= 'val_loss', #monitor= CHOOSE_METRIC_TO_MONITOR_FOR_PERFORMANCE, \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode= 'min', #mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC,\n",
    "                             save_weights_only = True)\n",
    "#checkpoint_2 = ModelCheckpoint(weight_path_acc, \n",
    "#                             monitor= 'val_acc', #monitor= CHOOSE_METRIC_TO_MONITOR_FOR_PERFORMANCE, \n",
    "#                             verbose=1, \n",
    "#                             save_best_only=True, \n",
    "#                             mode= 'max', #mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC,\n",
    "#                             save_weights_only = True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=decrease_rate, patience=4, min_lr=limit_rate)\n",
    "\n",
    "\n",
    "early = EarlyStopping(monitor= 'val_loss',  #monitor= SAME_AS_METRIC_CHOSEN_ABOVE,\n",
    "                       mode= 'auto', #mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC,\n",
    "                       patience=6)\n",
    "\n",
    "callbacks = [checkpoint_1, reduce_lr,early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = Adam(lr=0.0001, decay=0.00005)\n",
    "#optimizer = Adam(lr=learning_rate)\n",
    "optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "#optimizer = Adam()\n",
    "#optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "net.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#epochs = 20\n",
    "epochs = 25\n",
    "\n",
    "history = net.fit(train_data, \n",
    "                  steps_per_epoch = train_steps,\n",
    "                  validation_data=val_data,\n",
    "                  validation_steps=val_steps,\n",
    "                  epochs = epochs, \n",
    "                  class_weight=class_weights_dict,\n",
    "                  callbacks = callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Access additional metrics if present\n",
    "# additional_metric = history.history['your_additional_metric']\n",
    "#epochs = 15\n",
    "# Example: Plotting training and validation loss over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_history(history):\n",
    "    n = len(history.history[\"loss\"])\n",
    "    plt.figure (figsize = (12,10))\n",
    "    plt.plot(np.arange(0, n),history.history[\"loss\"], label='Training Loss')\n",
    "    plt.plot(np.arange(0, n),  history.history[\"val_loss\"], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    filename = '{}_{}_{}_loss.jpg'.format(model_name,version,parameters)\n",
    "    # Save the figure as a JPEG\n",
    "    plt.savefig(filename, format='jpg', dpi=300)  # Adjust dpi as needed\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Access additional metrics if present\n",
    "# additional_metric = history.history['your_additional_metric']\n",
    "#epochs = 15\n",
    "# Example: Plotting training and validation loss over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_acc_history(history):\n",
    "    n = len(history.history[\"accuracy\"])\n",
    "    plt.figure (figsize = (12,10))\n",
    "    plt.plot(np.arange(0, n),  history.history[\"accuracy\"], label='Accuracy')\n",
    "    plt.plot(np.arange(0, n),  history.history[\"val_accuracy\"], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    filename = '{}_{}_{}_acc.jpg'.format(model_name,version,parameters)\n",
    "\n",
    "    plt.legend()\n",
    "    # Save the figure as a JPEG\n",
    "    plt.savefig(filename, format='jpg', dpi=300)  # Adjust dpi as needed\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(history)\n",
    "plot_acc_history(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After training for some time, look at the performance of your model by plotting some performance statistics:\n",
    "\n",
    "Note, these figures will come in handy for your FDA documentation later in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_label(label, categories):\n",
    "  \"\"\"\n",
    "  One-hot encodes a single or a list of categorical labels.\n",
    "\n",
    "  Args:\n",
    "      label: A single categorical label (e.g., 'A') or a list of labels (e.g., ['A', 'B']).\n",
    "      categories: A list of possible categories (e.g., ['A', 'B']).\n",
    "\n",
    "  Returns:\n",
    "      A single one-hot encoded vector (if a single label is provided) or\n",
    "      a list of one-hot encoded vectors (if a list of labels is provided).\n",
    "  \"\"\"\n",
    "\n",
    "  if isinstance(label, list):\n",
    "    # Handle list of labels\n",
    "    if not all(isinstance(l, str) for l in label):\n",
    "      raise ValueError(\"Elements in labels must be strings\")\n",
    "    one_hot_encoded = []\n",
    "    for l in label:\n",
    "      if l not in categories:\n",
    "        raise ValueError(f\"Invalid label: {l}\")\n",
    "      one_hot_vector = np.zeros(len(categories))\n",
    "      one_hot_vector[categories.index(l)] = 1\n",
    "      one_hot_encoded.append(one_hot_vector.tolist())\n",
    "    return one_hot_encoded\n",
    "  else:\n",
    "    # Handle single label\n",
    "    if not isinstance(label, str):\n",
    "      raise ValueError(\"label must be a string\")\n",
    "    if label not in categories:\n",
    "      raise ValueError(f\"Invalid label: {label}\")\n",
    "    one_hot_vector = np.zeros(len(categories))\n",
    "    one_hot_vector[categories.index(label)] = 1\n",
    "    return one_hot_vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_prediction(predictions, categories):\n",
    "  \"\"\"\n",
    "  One-hot encodes predicted labels from a softmax output.\n",
    "\n",
    "  Args:\n",
    "      predictions: A list of probability distributions (one for each sample),\n",
    "                   where each element represents the probability of belonging to\n",
    "                   a specific class. Each probability distribution can be a NumPy\n",
    "                   array or a list.\n",
    "\n",
    "  Returns:\n",
    "      A list of one-hot encoded vectors, where each vector corresponds\n",
    "      to the one-hot encoded prediction for the corresponding sample.\n",
    "  \"\"\"\n",
    "\n",
    "  if not isinstance(predictions[0], (list, np.ndarray)):\n",
    "    raise ValueError(\"predictions must be a list of probability distributions\")\n",
    "\n",
    "  num_classes = len(categories)  # Assuming all distributions have same length\n",
    "\n",
    "  one_hot_encoded = []\n",
    "  for pred in predictions:\n",
    "    if len(pred) != num_classes:\n",
    "      raise ValueError(f\"Expected predictions to have {num_classes} elements, got {len(pred)}\")\n",
    "    predicted_class_index = np.argmax(pred)  # Find the index of the highest probability\n",
    "    one_hot_vector = np.zeros(num_classes)\n",
    "    one_hot_vector[predicted_class_index] = 1\n",
    "    one_hot_encoded.append(one_hot_vector.tolist())  # Convert to list if needed\n",
    "\n",
    "  return np.array(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_arrays(arrays):\n",
    "  \"\"\"\n",
    "  Merges multiple NumPy arrays of the same shape (64, 224, 224, 3) along the first dimension.\n",
    "\n",
    "  Args:\n",
    "      arrays (list): A list of NumPy arrays to be merged.\n",
    "\n",
    "  Returns:\n",
    "      numpy.ndarray: The merged array.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If the input arrays do not have the same shape.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if all arrays have the same shape\n",
    "  if not all(arr.shape == arrays[0].shape for arr in arrays):\n",
    "    raise ValueError(f\"All input arrays must have the same shape {arr.shape} , {arrays[0].shape}.\")\n",
    "\n",
    "  # Concatenate the arrays along the first dimension\n",
    "  merged_array = np.concatenate(arrays, axis=0)\n",
    "\n",
    "  return merged_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valXs = []\n",
    "valYs = []\n",
    "test_batch_number = 10\n",
    "\n",
    "for i in range(test_batch_number):\n",
    "    valX, valY = next(val_data)\n",
    "    valXs.append(valX)\n",
    "    valYs.append(valY)\n",
    "    \n",
    "Xs_test= merge_arrays(valXs)\n",
    "Ys_test = merge_arrays(valYs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_test, Ys_test = next(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Xs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After training, make some predictions to assess your model's overall performance\n",
    "## Note that detecting pneumonia is hard even for trained expert radiologists, \n",
    "## so there is no need to make the model perfect.\n",
    "model_path = '/workspace/home/{}_{}_{}.json'.format(model_name,version,parameters)\n",
    "\n",
    "\n",
    "# Load the JSON file\n",
    "with open(model_path, \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "# Create a new model from the JSON\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#test_batch_size = batch_size * test_batch_number\n",
    "#weight_path = '/workspace/home/VGG_0_m_loss.best.hdf5'\n",
    "weight_path_loss=\"/workspace/home/{}_{}_{}_loss.best.hdf5\".format(model_name,version,parameters)\n",
    "\n",
    "loaded_model.load_weights(weight_path_loss)\n",
    "#y_pred = np.argmax(loaded_model.predict(valX, batch_size = 64, verbose = True),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stat = loaded_model.predict(Xs_test, batch_size = 64, verbose = True)\n",
    "y_pred = np.argmax(y_pred_stat,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some examples of true vs. predicted with our best model: \n",
    "\n",
    "# Todo\n",
    "\n",
    "fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
    "i = 0\n",
    "threshold = 0.3\n",
    "for (c_x, c_y, c_ax) in zip(Xs_test[0:100], Ys_test[0:100], m_axs.flatten()):\n",
    "     c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "     print(c_y)   \n",
    "     if c_y[1] == 1: \n",
    "         if y_pred_stat[i][1] > threshold:\n",
    "             c_ax.set_title('1, 1')\n",
    "         else:\n",
    "             c_ax.set_title('1, 0')\n",
    "     else:\n",
    "         if y_pred_stat[i][0] > threshold: \n",
    "             c_ax.set_title('0, 1')\n",
    "         else:\n",
    "             c_ax.set_title('0, 0')\n",
    "     c_ax.axis('off')\n",
    "     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming your labels and predictions are NumPy arrays\n",
    "y_true = Ys_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot or analyze the ROC curve here\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "filename=\"/workspace/home/{}_{}_{}_ROC.jpg\".format(model_name,version,parameters)\n",
    "#plt.savefig(filename, format='jpg', dpi=300)  # Adjust dpi as needed\n",
    "plt.show()\n",
    "plt.savefig(filename, format='jpg', dpi=300)  # Adjust dpi as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')  # Annotate with values, format as integers\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "filename=\"/workspace/home/{}_{}_{}_CM.jpg\".format((model_name,version,parameters)\n",
    "plt.savefig(filename, format='jpg', dpi=300)  # Adjust dpi as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_true,y_pred_stat[:,1])\n",
    "#precision = precision_score(y_true, y_pred)\n",
    "#recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred,target_names = [\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_tresh(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], 'b--', label='precision')\n",
    "    plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    filename=\"/workspace/home/{}_{}_{}_precision_threshol_t.jpg\".format(model_name,version,parameters)\n",
    "    plt.savefig(filename, format='jpg', dpi=300)  # Adjust dpi as needed\n",
    "\n",
    "\n",
    "plot_precision_recall_vs_tresh(precision, recall, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall)\n",
    "print(precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the threshold where recall is 0.8\n",
    "recall_value = 0.75\n",
    "idx = (np.abs(recall - recall_value)).argmin() \n",
    "print('Precision is: '+ str(precision[idx]))\n",
    "print('Recall is: '+ str(recall[idx]))\n",
    "print('Threshold is: '+ str(thresholds[idx]))\n",
    "print('F1 Score is: ' +str(2 * (precision[idx] * recall[idx]) / (precision[idx] + recall[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the threshold where recall is 0.8\n",
    "precision_value = 0.75\n",
    "idx = (np.abs(precision - precision_value)).argmin() \n",
    "print('Precision is: '+ str(precision[idx]))\n",
    "print('Recall is: '+ str(recall[idx]))\n",
    "print('Threshold is: '+ str(thresholds[idx]))\n",
    "print('F1 Score is: ' +str(2 * (precision[idx] * recall[idx]) / (precision[idx] + recall[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred,target_names = [\"Negative\", \"Positive\"]))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
